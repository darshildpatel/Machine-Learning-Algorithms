{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Assignment 2\n",
    "### Progamming Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing KNN using intersection Distance and Jaccard Distance\n",
    "### Submitted By:- Ayush Sethi (as11500) , Darshil Patel (ddp337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def inter(c1, c2):\n",
    "    distance = 0\n",
    "    token1 = c1.split()\n",
    "    token2 = c2.split()\n",
    "\n",
    "    token1 = distinct(token1)\n",
    "    token2 = distinct(token2)\n",
    "    for tok in token1:\n",
    "        if tok in token2:\n",
    "            distance = distance +1\n",
    "    \n",
    "    if distance == 0:\n",
    "        return 1000\n",
    "    \n",
    "    return 1/distance\n",
    "\n",
    "def distinct(token1):\n",
    "    dist_token = []\n",
    "    for i in token1:\n",
    "        if i not in dist_token:\n",
    "            dist_token.append(i)\n",
    "    return dist_token\n",
    "\n",
    "def majority(a):\n",
    "    ones = 0\n",
    "    two = 0\n",
    "    for i in a:\n",
    "        if i == '1':\n",
    "            ones = ones + 1\n",
    "        else:\n",
    "            two = two + 1\n",
    "    if ones == two:\n",
    "        ones = ones +1\n",
    "            \n",
    "    return 1 if ones>two else 0\n",
    "\n",
    "def confusion_matrix(true, pred):\n",
    "    true_positive = 0\n",
    "    false_negative = 0\n",
    "    false_positive = 0\n",
    "    true_negative = 0\n",
    "    for i,j in zip(true,pred):\n",
    "        if int(i) == 1 and int(i) == j:\n",
    "            true_positive = true_positive + 1\n",
    "        if int(i) == 0 and int(i) == j:\n",
    "            true_negative = true_negative + 1\n",
    "        if int(i) == 1 and int(i) != j:\n",
    "            false_negative = false_negative + 1\n",
    "        if int(i) == 0 and int(i) != j:\n",
    "            false_positive = false_positive + 1\n",
    "    \n",
    "    mat = [[true_positive,false_negative],[false_positive,true_negative]]\n",
    "    accuracy = (true_positive + true_negative)/(true_positive + false_negative + false_positive + true_negative)\n",
    "    print(accuracy)\n",
    "    return mat, accuracy\n",
    "\n",
    "def jaccard(a, b):\n",
    "    c = a & b\n",
    "    return 1-float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "def myFunc1(test_comment, train_comment):\n",
    "    docA = test_comment\n",
    "    docB = train_comment\n",
    "    bowA = set(docA.split())\n",
    "    bowB = set(docB.split())        \n",
    "    return jaccard(bowA, bowB)\n",
    "\n",
    "def knn(test_comment,k,train,func):\n",
    "    dist_vector = [(func(test_comment, row.comment), row.comment, row.labels, row.idx) for index, row in train.iterrows()]\n",
    "    dist_vector = pd.DataFrame(dist_vector,columns = ['dist','comment','labels','idx'])\n",
    "    best_dist = dist_vector.nsmallest(k,'dist').dist\n",
    "    \n",
    "    best_dist = best_dist.tolist()\n",
    "    best_labels = [vector[2] for index, vector in dist_vector.iterrows() if vector[0] in best_dist]\n",
    "    return majority(best_labels)\n",
    "\n",
    "\n",
    "def cross_validation(full_set,folds, k, func):\n",
    "    avg = len(full_set) / float(folds)\n",
    "    out = []\n",
    "    last = 0.0\n",
    "    CV_accuracy = 0\n",
    "#     correct_prediction = 0\n",
    "#     total_prediction = 0\n",
    "\n",
    "    while last < len(full_set):\n",
    "        out.append(full_set[int(last):int(last + avg)])\n",
    "        last += avg\n",
    "\n",
    "    #print(out)\n",
    "    for fold in range(len(out)):\n",
    "        print(\"trying fold:\",fold)\n",
    "        training  = pd.DataFrame()\n",
    "        frames = [training]\n",
    "        \n",
    "        for i in range(len(out)):\n",
    "            if i!= fold:\n",
    "                frames.append(out[i])\n",
    "                \n",
    "        training = pd.concat(frames)                \n",
    "        prediction = []\n",
    "        for i in range(len(out[fold])):\n",
    "            prediction.append(knn(out[fold].iloc[i].comment, k, training, func))\n",
    "        \n",
    "        matrix, accuracy = confusion_matrix(out[fold].labels, prediction)\n",
    "#         correct_prediction = correct_prediction + matrix[0][0] + matrix[1][1]\n",
    "#         total_prediction = total_prediction + matrix[0][0] + matrix[0][1] + matrix[1][0] + matrix[1][1]\n",
    "        CV_accuracy = CV_accuracy + accuracy\n",
    "     \n",
    "    return CV_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('reviewstrain.txt', sep = \"/n\", header = None, engine = 'python')\n",
    "train['labels'] = train[0].str[0:1]\n",
    "train['comment'] = train[0].str[1:]\n",
    "train['idx'] = train.index\n",
    "train.drop(0,axis=1,inplace=True)\n",
    "#train_token = [list(set(row.comment.split())) for index, row in train.iterrows()]\n",
    "\n",
    "test = pd.read_csv('reviewstest.txt', sep = \"/n\", header = None, engine = 'python')\n",
    "test['labels'] = test[0].str[0:1]\n",
    "test['comment'] = test[0].str[1:]\n",
    "test['idx'] = test.index\n",
    "test.drop(0,axis=1,inplace=True)\n",
    "#test_token = [list(set(row.comment.split())) for index, row in test.iterrows()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) For k = 1, what is the predicted label for the following example in the test set: It\n",
    "## leaves little doubt that Kidman has become one of our best actors ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn(test.iloc[17].comment, 1, train, inter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2&3 What is the confusion matrix (on the test set) for k = 1?\n",
    "## iii. Report the accuracy, the true positive rate, and the false positive rate, on the test\n",
    "## set for k = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.604\n",
      "Confusion Matrix [[209, 64], [134, 93]]\n",
      "Accuracy: 0.604\n",
      "TRUE POSITIVE RATE 0.7655677655677655\n",
      "FALSE POSITIVE RATE 0.5903083700440529\n"
     ]
    }
   ],
   "source": [
    "prediction = []\n",
    "for i in range(len(test)):\n",
    "    prediction.append(knn(test.iloc[i].comment,1, train,inter))\n",
    "\n",
    "matrix, accuracy = confusion_matrix(test.labels,prediction)\n",
    "print(\"Confusion Matrix\", matrix)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "tpr = matrix[0][0]/(matrix[0][0] + matrix[0][1])\n",
    "print(\"TRUE POSITIVE RATE\", tpr)\n",
    "fpr = matrix[1][0]/(matrix[1][0] + matrix[1][1])\n",
    "print(\"FALSE POSITIVE RATE\", fpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  For k = 5, what is the predicted label for the following example in the test set: It\n",
    "## leaves little doubt that Kidman has become one of our best actors .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn(test.iloc[17].comment,5, train, inter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the confusion matrix (on the test set) for k = 5?\n",
    "## vi. Report the accuracy, the true positive rate, and the false positive rate, on the test set for k = 5.\n",
    "## vii. What is the accuracy on the test set for k = 5?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.606\n",
      "Confusion Matrix [[212, 61], [136, 91]]\n",
      "Accuracy: 0.606\n",
      "TRUE POSITIVE RATE 0.7765567765567766\n",
      "FALSE POSITIVE RATE 0.5991189427312775\n"
     ]
    }
   ],
   "source": [
    "prediction = []\n",
    "for i in range(len(test)):\n",
    "    prediction.append(knn(test.iloc[i].comment,5, train, inter))\n",
    "\n",
    "matrix, accuracy = confusion_matrix(test.labels,prediction)\n",
    "print(\"Confusion Matrix\", matrix)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "tpr = matrix[0][0]/(matrix[0][0] + matrix[0][1])\n",
    "print(\"TRUE POSITIVE RATE\", tpr)\n",
    "fpr = matrix[1][0]/(matrix[1][0] + matrix[1][1])\n",
    "print(\"FALSE POSITIVE RATE\", fpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## viii. Suppose we used the very simple Zero-R classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.546\n",
      "([[273, 0], [227, 0]], 0.546)\n"
     ]
    }
   ],
   "source": [
    "# Zero R classifier\n",
    "# 1 is most common\n",
    "df_filtered = train[(train.labels == '1')]\n",
    "len(df_filtered)\n",
    "\n",
    "prediction = []\n",
    "for i in range(len(test)):\n",
    "    prediction.append(knn(test.iloc[i].comment, 5, train, inter))\n",
    "\n",
    "print(confusion_matrix(test.labels,[1]*500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for best K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying fold: 0\n",
      "0.6833333333333333\n",
      "trying fold: 1\n",
      "0.6766666666666666\n",
      "trying fold: 2\n",
      "0.6633333333333333\n",
      "trying fold: 3\n",
      "0.63\n",
      "trying fold: 4\n",
      "0.6466666666666666\n",
      "trying fold: 0\n",
      "0.65\n",
      "trying fold: 1\n",
      "0.6733333333333333\n",
      "trying fold: 2\n",
      "0.71\n",
      "trying fold: 3\n",
      "0.6533333333333333\n",
      "trying fold: 4\n",
      "0.6033333333333334\n",
      "trying fold: 0\n",
      "0.6366666666666667\n",
      "trying fold: 1\n",
      "0.5966666666666667\n",
      "trying fold: 2\n",
      "0.6633333333333333\n",
      "trying fold: 3\n",
      "0.6066666666666667\n",
      "trying fold: 4\n",
      "0.5566666666666666\n",
      "(3, 3.3)\n"
     ]
    }
   ],
   "source": [
    "list_K = [3, 7, 99]\n",
    "def findBestK(list_K):\n",
    "    K_accuracy = dict()\n",
    "    for i in list_K:\n",
    "        K_accuracy[i] = cross_validation(train, 5, i, inter)\n",
    "    \n",
    "    #print(K_accuracy)    \n",
    "    return min(sorted(K_accuracy.items(), key=lambda x: x[1]))  \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "print(findBestK(list_K)) \n",
    "\n",
    "best_K = 3\n",
    "# 3 Has the highset Accuracy\n",
    "# {3: 0.6599999999999999, 7: 0.658, 99: 0.612}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy on highest k using test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.59\n",
      "Confusion Matrix [[212, 61], [144, 83]]\n",
      "Accuracy: 0.59\n"
     ]
    }
   ],
   "source": [
    "prediction = []\n",
    "for i in range(len(test)):\n",
    "    prediction.append(knn(test.iloc[i].comment,best_K, train, inter))\n",
    "\n",
    "matrix, accuracy = confusion_matrix(test.labels,prediction)\n",
    "print(\"Confusion Matrix\", matrix)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With New distance function\n",
    "\n",
    "## k = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.642\n",
      "Confusion Matrix [[206, 67], [112, 115]]\n",
      "Confusion Matrix 0.642\n"
     ]
    }
   ],
   "source": [
    "prediction = []\n",
    "for i in range(len(test)):\n",
    "    prediction.append(knn(test.iloc[i].comment,1, train, myFunc1))\n",
    "\n",
    "matrix, accuracy = confusion_matrix(test.labels,prediction)\n",
    "print(\"Confusion Matrix\", matrix)\n",
    "print(\"Confusion Matrix\", accuracy)\n",
    "tpr = matrix[0][0]/(matrix[0][0] + matrix[0][1])\n",
    "print(\"TRUE POSITIVE RATE\", tpr)\n",
    "fpr = matrix[1][0]/(matrix[1][0] + matrix[1][1])\n",
    "print(\"FALSE POSITIVE RATE\", fpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.668\n",
      "Confusion Matrix [[220, 53], [113, 114]]\n",
      "Confusion Matrix 0.668\n",
      "TRUE POSITIVE RATE 0.8058608058608059\n",
      "FALSE POSITIVE RATE 0.4977973568281938\n"
     ]
    }
   ],
   "source": [
    "prediction = []\n",
    "for i in range(len(test)):\n",
    "    prediction.append(knn(test.iloc[i].comment,5, train, myFunc1))\n",
    "\n",
    "matrix, accuracy = confusion_matrix(test.labels,prediction)\n",
    "print(\"Confusion Matrix\", matrix)\n",
    "print(\"Confusion Matrix\", accuracy)\n",
    "tpr = matrix[0][0]/(matrix[0][0] + matrix[0][1])\n",
    "print(\"TRUE POSITIVE RATE\", tpr)\n",
    "fpr = matrix[1][0]/(matrix[1][0] + matrix[1][1])\n",
    "print(\"FALSE POSITIVE RATE\", fpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
